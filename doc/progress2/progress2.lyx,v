head	1.5;
access;
symbols;
locks
	jules:1.5; strict;
comment	@# @;


1.5
date	2001.09.06.03.57.31;	author jules;	state Exp;
branches;
next	1.4;

1.4
date	2001.09.05.23.36.07;	author jules;	state Exp;
branches;
next	1.3;

1.3
date	2001.09.02.19.42.23;	author jules;	state Exp;
branches;
next	1.2;

1.2
date	2001.09.02.19.40.36;	author jules;	state Exp;
branches;
next	1.1;

1.1
date	2001.09.01.19.49.19;	author jules;	state Exp;
branches;
next	;


desc
@(no initial description)
@


1.5
log
@(no log message)
@
text
@#LyX 1.1 created this file. For more info see http://www.lyx.org/
\lyxformat 2.16
\textclass article
\language english
\inputencoding latin1
\fontscheme default
\graphics default
\paperfontsize default
\spacing single 
\papersize Default
\paperpackage a4
\use_geometry 0
\use_amsmath 0
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\quotes_times 2
\papercolumns 1
\papersides 1
\paperpagestyle default

\layout Title

Ph.D.
 report
\layout Author

Julian Brown
\layout Section

Background
\layout Standard

The research I intend to carry out is in the field of dynamic optimisation.
\layout Standard

To achieve this aim, a new instruction-set architecture (ISA) will be developed,
 with the specific aims of ease of program analysis, and with features to
 reduce the overhead for run-time code modification to a minimum.
\layout Standard

The starting point of this research is in an emulation system, which though
 not trying to solve exactly the same problem, has given a fair amount of
 insight into what the problems are in trying to analyse and optimise code
 as it is running.
 An overview of this system is given below for informational purposes, and
 to give some backup to the reasoning behind some of the new architecture
 features I am proposing.
\layout Standard

The work which needs to be completed is as follows.
 Firstly, the new architecture (provisionally named 
\begin_inset Quotes eld
\end_inset 

Chameleon
\begin_inset Quotes erd
\end_inset 

) needs to be finalised (a draft version is described below).
 Secondly, a toolchain needs to be developed for the purposes of sourcing
 binaries for the architecture.
 It is anticipated that suitable additions to the `gcc' compiler can be
 made.
 Also, an execution environment needs to be developed, which is accurate
 enough to model the estimated performance of a hardware implementation
 of Chameleon.
 Only then can dynamic optimisation techniques be tested and developed.
\layout Standard

It will be useful to try to duplicate as little work as possible before
 that point is reached.
 Existing projects such as SimpleScalar
\begin_float footnote 
\layout Standard

http://www.simplescalar.com/
\end_float 
 or SimOS might be exploited to avoid implementing an entire virtual machine.
\layout Section

Dynamic Optimisation
\layout Subsection

Dynamic vs profile-driven optimisation
\layout Standard

There has been much focus recently on profile-driven optimisation techniques.
 In fact, certain new processor designs are practically dependent on such
 techniques to achieve their best performance (eg, Intel's Itanium architecture
 (ref!)).
\layout Standard

There are two main problems with the profile-driven approach.
 The first is that a program must be run with a set of `typical' data at
 compile time, and such data is not always available, or the program will
 be run on such diverse data that it is not possible to pinpoint a particular
 data set to optimise on.
 The second problem is more pragmatic, in that it is difficult to persuade
 developers to develop and ship code using profile-driven compilation, since
 it adds another two (potentially time-consuming) stages to the build process.
 Moreover, problems with compilers in the past have led to much commercial
 software being shipped with optimisations turned off altogether (ref!).
\layout Standard

Dynamic optimisation solves each of these problems.
 In the latter case, developers do not need to perform extra profile-gathering
 and optimisation passes.
 In the former case, since code can adapt to whatever data it is running
 on at the time, there is no need to create contrived `typical' data sets.
 Even data which changes in character as a program is running can be handled,
 as the program may adapt to those changes.
\layout Subsection

Existing work
\layout Standard

An amount of work has been done on various related topics, feedback-directed
 optimisation, dynamic optimisation, dynamic recompilation.
 Some projects are briefly dissected below.
\layout Subsubsection

Dynamo
\layout Standard

One of the best-documented attempts at dynamic optimisation is Hewlett Packard's
 Dynamo.
 The aim of their research is to speed up native HP/UX binaries running
 on PA-RISC powered workstations.
\layout Standard

This was not the original aim of the project.
 They too were investigating how binary translation can be used to execute
 non-native binaries at native speeds.
 That is, reading in code from a particular ISA, translating to an internal
 form, optimising then translating back to a different ISA.
 For ease of implementation, they chose to target the same ISA, and found
 that native binaries would sometimes actually improve in execution speed,
 sometimes by 20% or more.
\layout Standard

Dynamo executes entirely in user space on unmodified hardware.
 Much like ARMphetamine, it starts out by always 
\emph on 
emulating
\emph default 
 the code it's running.
 It profiles code as it runs it, building up a 
\emph on 
fragment cache
\emph default 
 of sections of code which are executed frequently.
 These fragments are `traces' from the original execution of the code --
 that is, they linearise complicated control flow, turning it into straight-line
 code.
 This can be handled more efficiently by the CPU's prefetch hardware and
 instruction cache.
\layout Standard

The other way Dynamo gains an advantage over statically-optimised code is
 by using its own lightweight optimiser.
 Since optimisation is done at run-time, optimisations themselves must be
 done very quickly.
 The optimisations attempted include optimisations across procedure call/return
 boundaries and indirect branches or virtual function calls.
\layout Standard

Since Dynamo operates on native binaries, if at any point it decides that
 too much time is being spent in re-optimisation, it can ``bail out'' and
 return control to the native processor.
 This obviously isn't possible if non-native binaries are being used.
\layout Subsubsection

Spike
\layout Subsubsection

Morph
\layout Subsubsection

Jalapeno
\layout Subsubsection

Transmeta Crusoe
\layout Standard

Transmeta's Crusoe processor takes steps beyond Dynamo in several important
 respects.
 It adds support for exceptions and speculation (shadow registers), optimization
 of memory operations (alias hardware), and self-modifying code (a ``translated"
 bit in the MMU).
 And of course it works on non-native binaries, executing IA-32 code on
 a VLIW processor core.
\layout Standard

Crusoe's method of operation is otherwise similar to Dynamo's.
 It starts out emulating code, then for frequently executed sections it
 does a `rough' translation to its own native (VLIW) code.
 These translations are cached and further optimised over time the more
 they are executed.
 By this method the designers claim that performance comparable to a modern
 hardware IA-32 implementation is obtained.
\layout Subsection

Limitations of a software-only emulation approach
\layout Standard

I started this research aiming to apply dynamic optimisation techniques
 to a dynamically recompiling emulator (for an ARM chip on an IA-32 platform).
 When writing this sort of emulator (my attempts are described below), one
 is not really modelling accurately what a processor actually does.
 In fact, it's more like designing an 
\emph on 
implementation
\emph default 
 of an instruction-set architecture, perhaps for the purpose of running
 an operating system designed for that architecture on a foreign machine,
 rather than, say, coverification of additional hardware models.
 In many cases, we can fairly safely take short-cuts on issues such as perfect
 cycle-timing accuracy without compromising our ability to run the operating
 system and its applications, if only because different real processor implement
ations might also have different cycle timings for instructions (eg, Intel's
 vs AMD's IA-32 offerings), so the operating system is written to perform
 timing using other means.
 We still encounter problems when attempting to run system hardware timers,
 used for dealing with I/O etc., which have to be produced in a timely manner
 whether code is being translated or run in a recompiled form.
\layout Standard

What we must not do is let the state of the virtual machine diverge from
 the state of the equivalent real machine.
 For example, if a leaf subroutine affects the processor flags or a `scratch'
 register, we cannot optimise that calculation away, even if the context
 in which that leaf subroutine is called is insensitive to the result of
 that calculation.
 We cannot be sure that the subroutine is not called from elsewhere in such
 a way that the calculation is vital.
 This seems obvious, but severely limits the kind of optimisations we can
 perform.
 For example, say we were translating code from some hypothetical CISC processor
 with eight general-purpose registers available for data processing.
 In many cases these are not sufficient to hold all the temporary values
 necessary to perform a calculation, so some of them must instead be stored
 in memory locations, say on a stack.
 If we are translating this code to some RISC architecture with 32 available
 registers, it seems desirable to assign some of the stack locations to
 registers instead, to make better use of the resources available.
 The anticipated problems with this are as follows:
\layout Standard

We are looking at the code through a potentially very small `window'.
 This leads to a bad case of the `pointer aliasing' problem, whereby we
 can't tell if loads or stores to particular memory locations will affect
 loads or stores from other memory locations.
 If we factor out a store and a load from what we assume to be the stack,
 and place the value in a register for the duration, an intermediate load
 or store which should affect that value, will not.
 Transmeta used additional hardware to help work around this problem, which
 is not available if we take a software-only approach.
 Note it doesn't matter if such register reallocation would work 
\emph on 
most of the time
\emph default 
 -- if it will sometimes fail we simply can't do it.
\layout Standard

Similarly, we can't tell (or at least, we probably can't tell) if a particular
 load or store points at some volatile memory location, such as I/O space
 or a memory location which is touched by code running under some sort of
 interrupt.
 Also similarly, most of the time we could probably make assumptions, but
 the rare cases in which we are wrong would lead to unacceptable program
 behaviour.
 Of course, anything that can be done in hardware can also be done in software,
 it will just take longer.
 If the emulated memory system implements paging in software, it may well
 be faster to sometimes use explicit aliasing checks.
\layout Standard

Exceptions are another problem.
 Exceptions can be divided into two categories, those which occur asynchronously
, where the program counter location is of little consequence (eg, keyboard
 interrupts), and those which occur synchronously, such as page faults.
 In the latter case, it's vital that the original state of the processor
 can be recovered, so we can find exactly where the fault occurred and continue
 from that point.
 This means that effectively, we cannot schedule instructions to make better
 use of memory bandwidth and available execution units.
 Again, Transmeta have solved this problem with extra hardware (rollback
 registers).
\layout Subsection

Possible optimisations
\layout Standard

Transmeta's Crusoe processor, as mentioned, overcomes some of these problems
 using special hardware.
 However, the optimisations they can perform are still limited in scope
 by the necessity of using IA-32 as their source ISA.
\layout Standard

It's worth taking a step back and looking at what might be possible if we
 weren't constrained by modelling an existing architecture, but instead
 designed a new ISA specifically tailored for the demands of dynamic optimisatio
n.
 In particular, we would like code which is exceptionally easy to analyse.
 In addition, we should retain certain information in the instruction stream
 regarding loops, functions and variable liveness which are not normally
 present in binary images.
\layout Standard

As an idea of the kind of thing we'd hope would be possible with such an
 ISA, here are a few sample optimisations which could be performed.
\layout Itemize

Shared-library stub bypassing.
 This should be an obvious, easy optimisation.
 Dynamic-linked libraries normally link references to an external object
 through some sort of jump table, adding an extra level of call overhead
 per function call in the shared library.
 
\layout Itemize

Inlining of `hot' functions.
 Going one step beyond removing a layer of call overhead to a function,
 the function might actually be inserted directly into the code in the necessary
 places.
 There are obviously trade-offs with code expansion vs any speedup gained,
 except perhaps in the case where very small leaf functions (eg, accessor
 functions in C++) take up less space than the original function-call sequence,
 in which case inlining is a no-brainer.
\layout Itemize

Re-optimising around inlined functions:
\begin_deeper 
\layout Itemize

Once we've inlined a function, we can remove any code which, say, pushes
 arguments onto the stack only to pull them off again immediately.
 Any code dealing with stack frames can be removed.
\layout Itemize

Re-running common subexpression elimination, dead code removal.
 An inline function will often perform surplus work which might be unnecessary
 in a particular calling context
\end_deeper 
\layout Itemize

`Hot cold' optimisation.
 Given continuously-gathered profile information, we can rearrange functions
 so that all 
\begin_inset Quotes eld
\end_inset 

hot
\begin_inset Quotes erd
\end_inset 

 code executed in the common case is made contiguous, moving less-frequently
 executed 
\begin_inset Quotes eld
\end_inset 

cold
\begin_inset Quotes erd
\end_inset 

 code further away where it cannot pollute cache lines.
 
\begin_inset LatexCommand \cite{BL 94,CL 96}

\end_inset 


\layout Itemize

Partial evaluation.
 Continuously-gathered data profiling might enable specialised versions
 of functions to be generated, with some or all arguments replaced by constants.
 Run-time constants enable further optimisations such as loop unrolling
 to be performed.
 
\begin_inset LatexCommand \cite{LL 96}

\end_inset 


\layout Itemize

Automatic memoization.
 For slow functions, keep function return values in a cache, and use those
 values instead of recomputing the function
\begin_float footnote 
\layout Standard

This requires a function to have no side effects, the determination of which
 may be a stumbling point.
 Memoization might be the solution to a different problem, I'm not sure.
\end_float 
 
\begin_inset LatexCommand \cite{Michie 68}

\end_inset 

.
\layout Itemize

Data and code prefetching.
 Continuously-gathered profile information might enable prefetch instructions
 to pull data from main memory into caches in a timely fashion
\layout Itemize

Branch hinting.
 If a branch is taken 90% of the time, insert a hint to say so and code
 will be prefetched from the taken address.
\layout Itemize

Instruction scheduling.
 If execution unit utilisation is low for a piece of code, instruction schedulin
g might be re-run for that code
\begin_float footnote 
\layout Standard

This requires a simulation of the new architecture detailed enough to take
 into account multiple issue execution units and cycle accuracy.
 Given constraints on resources available, this may be impossible to achieve
\end_float 
.
 It might be interesting to investigate if using spare bits in the instruction
 encoding to specify which execution unit to use for the calculation could
 be an alternative to either the VLIW approach or to performing instruction
 despatch entirely in hardware.
\layout Itemize

Loop transformations.
 If nested loops turn out to cause poor cache utilisation, they can be transform
ed into loops with better characteristics.
 The amount of annotation carried over from a high-level source language
 would need to be quite high for this to work (refs!)
\layout Standard

Many of these optimisations are difficult or impossible to perform at run-time
 on a normal processor (even with an emulation layer like HP Dynamo uses),
 since there is too little information available in most instruction-set
 architectures to know what is safe to do, and when to do it.
\layout Subsection

Fun and games
\layout Standard

Here is an interesting take on the dynamic optimisation idea.
 Instead of viewing dynamic optimisation as an introspective form of dynamic
 recompilation, it can be viewed as an attempt to automatically perform
 the `trick' that a dynamic recompiler 
\emph on 
itself
\emph default 
 uses to speed up the code it is emulating, on any type of program which
 processes data.
\layout Standard

To elaborate, consider what a dynamically-recompiling emulator does to achieve
 its speed.
 At the basic level the hosted program is run by a software emulation of
 the original processor.
 `Hot' sections of code -- actually data -- are made into specialised subprogram
s which `process' themselves, so that the net result is the same as processing
 the data in the original, slow way.
 Except, of course, that the desired result is obtained much faster.
 Using some combination of the above, or other, techniques, this transformation
 from an `emulator' to a dynamic `compiler' for 
\emph on 
any type of program
\emph default 
 might be achieved.
 This sounds unlikely, but I am hopeful.
\layout Section

Chameleon architecture
\layout Standard

The Chameleon architecture is intended to enable as many of the previously-menti
oned optimisations as possible.
 It has a new instruction set designed specifically for ease of program
 analysis.
 Other features, such as code density and even amenabilty to particularly
 fast execution, have been compromised to some extent.
\layout Standard

Below, some of the features of the architecture (as anticipated at this
 point in time) are described.
\layout Subsection

Basics
\layout Standard

Chameleon is a 32-bit processor, with 64 registers.
 Each instruction has a fixed, 6-bit opcode in order to distinguish it from
 the others.
 Only scalar integer arithmetic is supported.
 The operations available use three operands, ie 
\begin_inset Quotes eld
\end_inset 

add r1, r2, r3
\begin_inset Quotes erd
\end_inset 

 will add r2 and r3, and put the result in r1.
 Data processing operations may use an immediate value for the second source
 operand.
\layout Subsection

Control flow
\layout Standard

Code formed into basic blocks.
 Basic blocks have meta-data.
 X and Y pointers.
 Different block types.
 Special registers.
\layout Subsection

Register liveness
\layout Standard

Start, end register bitmaps for blocks.
 Expiry coding in instructions.
\layout Subsection

Clean multi-precision arithmetic
\layout Standard

Extra bits in registers.
 Potential problem with tag bits & spill code.
\layout Subsection

Memory access features
\layout Standard

Multi-instruction load & store, instruction stream writing.
\layout Subsection

Merge this stuff
\layout Standard

Variable liveness information encoded in the instruction stream.
\layout Itemize

Novel control flow.
 Code is organised in basic blocks, which are tagged with various meta-data,
 such as the liveness of registers at the start and end of each block, a
 `type' field for each block, etc.
\layout Itemize

No addresses.
 Other basic blocks are referred to by references.
 All data is relocatable, and accessed through a base register which is
 set accordingly before each basic block is invoked
\begin_float footnote 
\layout Standard

This might be utterly unworkable if used with languages such as C.
 Needs further thought.
\end_float 
.
\layout Itemize

No special condition code registers.
 Like Alpha, normal registers are used to hold condition codes.
\layout Itemize

32-bit registers with additional tags.
 Registers `know' if they are alive at a particular point in a program's
 execution.
 There are per-register carry and overflow flags to deal with multi-precision
 arithmetic.
 This allows one `type' to be used for dataflow analysis throughout, hopefully.
\layout Itemize

Multi-register load & store instructions.
 Modelled after the ARM, but only allowing one contiguous range of registers
 to be stored.
 Reduces stack frame manipulation code to one or two instructions, without
 loss of generality.
\layout Itemize

Large number of (64) registers, most of which are general-purpose.
\layout Itemize

Four sets of fully-shadowing registers for cheap exception handling, and
 to allow code analysis to take place without altering the processor's state
\layout Standard

Some features are not fully decided, and probably will not be until further
 in the development process.
 These include:
\layout Itemize

How code and data profiling hardware should behave.
 It's anticipated that either special cache or an area of main memory might
 be set aside for gathering profile data on running code, but what data
 is gathered and how it is stored.
\layout Itemize

Special processor features, for example instructions which are able to directly
 decode other instructions, or instructions which are able to `half execute'
 instructions without changing external processor state or the contents
 of registers, but only tags associated with registers.
 The purpose of this would be to quickly perform certain optimisations,
 such as:
\begin_deeper 
\layout Itemize

Constant propagation.
 Registers are tagged with a `constant' bit.
 Operations on other constant registers yield further constant results.
 The logic should not be complicated from a hardware standpoint, but perhaps
 working out where (runtime) constants are would be.
\layout Itemize

Expression hashing for common subexpression elimination.
 Say performing the sequence add, multiply, subtract on certain registers
 would give a particular hash value, and performing a functionally-equivalent
 add, multiply and subtract (but perhaps in a different order, or with unrelated
 instructions interleaved) would produce the same hash value, a form of
 local CSE could be done fairly quickly.
 Combine this with the `hot' function inlining optimisation to gain the
 most benefit from doing this at run-time.
\end_deeper 
\layout Section

Outstanding issues
\layout Standard

Problems with C.
 Arbitrary pointer arithmetic.
 Producing compiled code.
 Limits of available resources.
\layout Section

Translating binary code from ARM to IA-32
\layout Standard

I've been working on a fast ARM emulator
\begin_float footnote 
\layout Standard

Named ARMphetamine, which is a very bad pun indeed.
 I've apologised about it before, so I won't do it again
\end_float 
, which uses dynamic recompilation from ARM binary code to IA-32 (x86) binary
 code to achieve high speeds
\begin_float footnote 
\layout Standard

ARMphetamine circa my final-year report was, to my knowledge, the speediest
 ARM emulator in existence, though it was 
\emph on 
slightly
\emph default 
 rough around the edges.
 The version described here can't actually run any code yet, but the code
 it generates looks a good deal better than it used to on the examples it
 translates correctly.
\end_float 
.
 Though it is unfinished, I describe this system below since it provides
 the background to the Chameleon project and serves to outline the difficulties
 of performing dynamic optimisation in the code-translation environment,
 and highlights some of the pitfalls which might otherwise be made.
\layout Standard

Translating machine code from one architecture to another is a fairly difficult
 problem.
 Though most processors perform many of the same operations at a basic level,
 the `interface' they present to the outside world can be very different,
 in terms of the bit-patterns or machine code which they use.
 These differences can be fairly major, such as whether fixed-length or
 variable-length instructions are used or the width of data operands, or
 fairly subtle such as whether performing a particular operation causes
 side-effects in the processor's state.
\layout Standard

In a complete system emulation environment, it is not possible (or even
 desirable) to translate `all' of the available binary code to the host
 architecture.
 Even in the simpler case of translating, say, an entire application binary,
 without extra information (which is not normally available) it is not possible
 to translate it wholly into a native binary -- because the analysing program
 cannot know what is code and what is data
\begin_float footnote 
\layout Standard

The solution is equivalent to the halting problem: to discover everywhere
 a program might go, we need to know everything the program can do (since
 we will be using calculated jumps in just about any non-trivial program).
 If we can discover everything a program can do, then we could determine
 whether there are any infinite loops in the code.
 We know we can't do the latter, so we also can't do the former.
\end_float 
.
 Hence any code translation should be done at emulation time, `dynamically',
 once we already know what has been executed.
\layout Standard

This brings problems of its own.
 Most obviously, the speed at which compilation is done now becomes a much
 more important issue than when dealing with a normal compiler, so special
 techniques must be developed which produce code quickly.
 Also, the `compilation window' is potentially smaller than is commonly
 used -- we can't optimise at the level of functions, because (in the general
 case) we don't know where the functions are.
 The thing which causes the most problems though is that binary code was
 never intended to be abused in the manner of treating it as source code,
 and issues such as the `boundary' cases of operations can cause immense
 trouble.
\layout Subsection

Profiling algorithm
\layout Standard

Code on the guest machine is emulated normally, by interpreting instructions
 sequentially.
 As this is done, profile information is gathered using a low-overhead technique
, so that a heap of contiguous chunks of code is gathered.
 If one of these chunks (which are indexed in a hash table by their 
\emph on 
single
\emph default 
 entry point) is emulated more than a certain number of times, then that
 chunk is translated and cached so that native code can be run in its place
 in the future.
\layout Standard

There are problem with such things as branches outside `known' code, but
 in those cases we can return control to the interpretive emulator, or insert
 hooks into the code to attach the previously-unknown code to if it gets
 recompiled at some point in the future.
\layout Subsection

Translation algorithm
\layout Standard

Given a piece of ARM code, we perform several passes on it in order to translate
 it to IA-32 code.
 Firstly, since each ARM instruction can be quite complicated, we translate
 to an intermediate representation
\begin_float footnote 
\layout Standard

Note that one of the things Chameleon is trying to avoid is the need for
 such an intermediate representation, by making the ISA 
\emph on 
itself
\emph default 
 the intermediate representation.
\end_float 
.
 which is much simpler and much easier to analyse.
 I will describe this intermediate representation (ph2) below, since it
 is an important part of ARMphetamine, and helped form many of the ideas
 behind the Chameleon instruction-set architecture
\begin_float footnote 
\layout Standard

Apart from some of the nasty bits, which aren't necessary if an existing
 processor isn't being emulated
\end_float 
.
\layout Standard

The important things we need to do with the intermediate representation
 are extracting and optimising control flow, performing register allocation
 and doing IA-32 code generation.
\layout Standard

Currently, the order of the passes required to do each of these is as follows:
\layout Itemize

Allocate constants.
 This means that ph2 variables which are loaded with constants need not
 have physical IA-32 registers attached to them later.
\layout Itemize

Optimise transitive branches.
 Rewrite the target of the first branch which points directly to a second
 branch of compatible type to the target of the second branch (
\begin_inset Quotes eld
\end_inset 

branch threading
\begin_inset Quotes erd
\end_inset 

).
\layout Itemize

Cull unused nodes.
 As possibly generated by the previous step, or earlier in the ARM->ph2
 translation step.
\layout Itemize

Depth-first search.
 Find `parent' nodes, start and finish times.
 Used by the strongly-connected component code later.
\layout Itemize

Shuffle commits.
 Move commits as early as possible, to relieve register pressure.
\layout Itemize

Find predecessors.
 This finds the inverse of the control flow graph.
\layout Itemize

Strongly-connected components.
 This finds strongly-connected components in the control flow graph.
\layout Itemize

Fix-up flags & predicates.
 Ensures that flags and control condition codes are stored when needed,
 and culls unnecessary flag calculation code.
\layout Itemize

Find spans.
 Find live variable ranges in ph2 code.
\layout Itemize

Source-destination alias.
 Where possible, alias the destination variable to the source variable of
 the three-address operations used by ph2.
 This is important for IA-32 code, where one of the source operands for
 calculations is usually overwritten by the result.
\layout Itemize

Re-run span finder.
 The aliases from the previous stage alter the variable liveness information.
\layout Itemize

Linear scan register allocation/code generation.
 Described below.
\layout Itemize

Insert load/store code.
 Cannot be known until after the previous stage has completed.
\layout Itemize

Finish allocation.
 The way register allocation works means some variables cannot be fully
 allocated until the end.
 This fixes them.
\layout Itemize

Flatten code.
 This takes abstract structure of IA-32 instructions, and flattens them
 into contiguous code.
\layout Standard

I will explain some of these stages further below, but first I will describe
 the ph2 intermediate language.
\layout Subsection

Intermediate representation
\layout Standard

The intermediate representation (ph2) is a three-address code, ie instructions
 are of the form:
\layout Standard

add %7, %3, %4
\layout Standard

%3, %4 and %7 represent variables.
 %3 and %4 are source operands, %7 is the destination.
 Each of the operations available are simple (simpler than many RISC chips).
 ARM code in particular can be quite expressive, allowing immediate and
 shifted operands for data-processing instructions, predicated execution
 of instructions, etc.
 Complex ARM instructions are broken up into many ph2 instructions.
 Additionally, the ARM instruction encoding at a bit level is fairly involved,
 and ph2's encoding is very simple.
 This aids code analysis greatly, and has been very influential on the Chameleon
 architecture.
\layout Subsubsection

SSA
\layout Standard

By convention, ph2 is always written in static single assignment (SSA) form.
 During the ARM->ph2 translation phase, this conversion is performed by
 performing
\emph on 
 register renaming
\emph default 
 in much the same way as a modern superscalar processor does.
 This form allows certain types of optimisation to be performed very quickly
 and easily (linear time vs quadratic time).
\layout Subsubsection

Control flow
\layout Standard

Ph2 instructions are grouped into blocks.
 There is no explicit branch instruction, but instead each block has a condition
 type, and pointers to a `true' block and a `false' block.
 It is not necessary to exactly mirror all of the condition-code flags which
 the source machine code used, since the host machine's native control-flow
 logic is used instead.
 Of course, flags must be calculated correctly before control returns to
 the interpretive emulator.
\layout Standard

In more detail, on the ARM architecture, as on many others, flow control
 is achieved using condition codes, which are combinations of particular
 flags in some status register.
 For example, a piece of code might want to jump somewhere if one register
 is greater than or equal to another register:
\layout Standard

cmp r4,r5
\layout Standard

bge somewhere
\layout Standard

The first instruction sets four flags, named Z (zero), N (negative), V (overflow
) and C (carry).
 The second tests a combination of those flags for the ``greater-than or
 equal'' condition, which is defined by ``N set and V set, or N clear and
 V clear", or more concisely as ``N==V".
\layout Standard

The two architectures in question (ARM and IA-32) both employ flags for
 controlling flow like this, but have quite different semantics for setting
 and testing those flags.
 On the ARM for example, setting flags is optional for ALU operations, and
 any instruction may be conditionally executed.
 In IA-32, virtually all instructions unconditionally set (or in some cases
 corrupt) the processor flags.
 Nevertheless, we can still utilise the native processor's flag-calculation
 logic in recompiled code, although great care must be taken.
\layout Standard

One approach (that used in an earlier version of ARMphetamine 
\begin_inset LatexCommand \cite{Brown 00}

\end_inset 

), is to save and restore flags individually, on-demand, in recompiled code.
 This was proven to work adequately, but produced bulky and slow code.
 The reason is mainly a shortcoming of IA-32, which makes it particularly
 difficult to alter individual bits corresponding to the S, Z and O flags
 in the flags register.
 The code sequence used -- not necessarily the best -- took between 9 and
 18 instructions to restore between one and four bits.
 When this was multiplied by the number of times such an operation was necessary
 in a chunk of code, this meant a fairly large proportion of code was dedicated
 to this type of code.
\layout Subsubsection

Condition-code abstraction
\layout Standard

A better approach is to group flags forming a condition code together, and
 treat them as an atomic unit (IA-32 provides convenient set
\emph on 
xx
\emph default 
 instructions which allow you to do just that).
 This means that the state of the flags is not mirrored exactly in recompiled
 code, and the on-demand scheme can no longer be utilised.
 Instead, when we encounter a conditional branch (say), we scan the ph2
 representation backwards (over block boundaries if necessary) until we
 find the instruction which last altered the flags corresponding to the
 relevant condition code.
 Then we insert an instruction which saves that condition code into a predicate
 buffer, and use that value rather than the condition flags when we decide
 whether to take the branch or not.
 Additionally, the pass which does this removes any redundant flag calculation
\begin_float footnote 
\layout Standard

Actually flag saving, since on IA-32 flags are calculated whether you want
 them to be or not.
 It's the code to store the flags to memory that takes up space.
\layout Standard

As you might imagine, the subtleties of this approach are more involved
 than I'm describing here.
 ARM and IA-32 treat the carry flag in an inverse sense from one another
 as the result of subtraction (including comparison) operations for instance,
 so we must always ensure that we're using the correct sense.
 If a compound condition code, ie, the unsigned lower-or-same (carry-clear
 or zero-set) test is used, then the 
\emph on 
host
\emph default 
 machine's sense of the carry flag must be used.
 Contrariwise, If the carry-set (unsigned higher-or-same) test is used,
 the 
\emph on 
guest
\emph default 
 machine's sense of the carry flag must be used.
\end_float 
 code.
\layout Standard

Dealing with an instruction set, either in the source or target architecture,
 which had no need for an implicit flag word would make translation much
 easier.
 Hence, Chameleon does not use an implicit flag word.
\layout Section

Modified Linear Scan
\layout Standard

Linear Scan is a fast register allocation technique 
\begin_inset LatexCommand \cite{PS}

\end_inset 

, developed as an alternative to allocation by graph colouring 
\begin_inset LatexCommand \cite{Chaitin et al}

\end_inset 

 for situations where compile-time is at a premium.
 Unfortunately in its basic form it is inconvenient for use in a dynamic
 recompilation system since information it requires whilst working is needed
 during other stages of code generation.
 First I'll explain how it normally works, then I'll describe my modification.
 (An earlier version of ARMphetamine used an ad-hoc register allocation
 technique similar in spirit to ``binpacking" 
\begin_inset LatexCommand \cite{THS 98}

\end_inset 

)
\layout Standard

The algorithm works as follows.
 It is assumed that program instructions can be laid out in some order,
 for instance the order in which the intermediate representation is laid
 out in memory or the order arising from a depth-first search.
\layout Standard

The intermediate representation is scanned and from it a set of 
\emph on 
live intervals
\emph default 
 for variables is inferred.
 Two live intervals interfere if they overlap.
 Given 
\emph on 
R
\emph default 
 available registers and a list of live intervals, the linear scan algorithm
 must allocate registers to as many intervals as possible, such that no
 two overlapping intervals are assigned the same register.
 If 
\begin_inset Formula \( n>R \)
\end_inset 

 live intervals overlap at any point, at least 
\begin_inset Formula \( n-R \)
\end_inset 

 of them must reside in memory.
\layout Standard

The number of overlapping intervals only changes at the start and end of
 an interval.
 Live intervals are stored in an list sorted by increasing start point,
 so the algorithm can progress quickly by skipping from one start interval
 to the next.
\layout Standard

At each step, the algorithm maintains a list of 
\emph on 
active
\emph default 
 live intervals, which have been assigned to registers.
 This list is kept in order of increasing end point.
 For each new interval, the algorithm scans 
\emph on 
active
\emph default 
 and removes any `expired' intervals which are no longer pertinent to the
 current position in the source program, leaving such interval's registers
 free for reallocation.
 Since this list is stored in increasing end-point order, scanning can stop
 when an end-point greater than the current new start point has been encountered.
\layout Standard

The worst case scenario, where the active list has length 
\begin_inset Formula \( R \)
\end_inset 

 and no intervals have been expired, is resolved by choosing one of the
 intervals from 
\emph on 
active
\emph default 
 or the new start interval.
 As a heuristic, the interval which ends furthest from the current point
 is spilled to memory.
 This information can be found quickly since 
\emph on 
active
\emph default 
 is sorted by increasing end point.
 Further analysis is available in 
\begin_inset LatexCommand \cite{PS}

\end_inset 

.
\layout Standard

Due to the context from which the `source' code is obtained in our dynamic
 recompilation environment, any memory access instruction (load or store)
 can cause a data abort exception.
 If this happens, we must be able to recover the original processor state
\begin_float footnote 
\layout Standard

There is no practical opportunity for returning control to the block which
 caused the exception and allowing the state to be recovered gracefully,
 since the true machine state must be available to the exception handler
 anyway
\end_float 
.
 Up to 
\begin_inset Formula \( R \)
\end_inset 

 ARM registers may reside in IA-32 registers at a point in a recompiled
 block.
 If an exception occurs, these must be spilled back to memory before the
 exception handler is invoked.
 This set of registers is known as linear scan is in operation, so rather
 than wastefully store the data somewhere, code is generated at the same
 time as registers are allocated (a similar philosophy is advocated in 
\begin_inset LatexCommand \cite{Appel 98}

\end_inset 

).
\layout Standard

The problem then is that an interval used by code which has already been
 generated cannot easily be spilled without altering already-generated code
 in which that interval was live.
 The solution to this is to generate code in an abstracted form, with mutable
 `slots' for the operands.
 These slots are only finalised when the linear scan algorithm has completed.
\layout Standard

This approach has additional benefits where a non-orthogonal instruction
 set is concerned (such as on IA-32), in that if a particular register is
 needed for some operation, then:
\layout Itemize

As we're generating code at the same time as allocating registers, we can
 tell if particular registers are needed in a timely way
\layout Itemize

The set of allocated registers can be `shuffled around' to enable the difficult
 instruction to use the registers it wants.
\layout Standard

Note that even on `clean' architectures, the procedure call convention usually
 passes and returns values in particular registers.
\layout Section

Instruction Selection
\layout Standard

We generate code by simply choosing IA-32 instructions which correspond
 to ph2 instructions one at a time (ARMphetamine v1 used a complicated tree-base
d matching method).
 Code is generated indirectly via an abstraction layer as mentioned above,
 which performs a similar task to a text-source assembler in choosing instructio
n variants for different operand types, but also knows about which instructions
 set up and destroy condition-code flags.
\layout Section

Flattening Code
\layout Standard

Given a graph of basic blocks, there are any number of ways of laying out
 those blocks into a linear instruction stream.
 My algorithm is fairly ad-hoc, and as yet has not been subjected to rigourous
 testing.
 A priority queue is used to decide which block to output next.
 The `true' and `false' blocks for each block by default are added to this
 queue after that block has been generated.
 Their priorities are increased if they are:
\layout Itemize

Part of the same strongly-connected component as the current block
\layout Itemize

For the true block, if there is no false block
\layout Itemize

In the future, a weighting might be added depending on how often branches
 were taken/not taken in the profiling of the original code before translation.
\layout Standard

The rationale being that spatially-adjacent code accesses are more likely
 to be accessed close together in time.
\layout Bibliography
\bibitem {CL 96}

R.
 Cohn and G.
 Lowney, 
\begin_inset Quotes eld
\end_inset 

Hot Cold Optimization of Large Windows/NT Applications,
\begin_inset Quotes erd
\end_inset 

 Proc.
 of the 29th Annual Int.
 Symposium on Microarchitecture, pages 80-89, December 1996.
\layout Bibliography
\bibitem {BL 94}

T.
 Ball and J.
 Larus, 
\begin_inset Quotes eld
\end_inset 

Optimally Profiling and Tracing Programs,
\begin_inset Quotes erd
\end_inset 

 ACM Transactions on Programming Languages and Systems, 16(3):1319-1360,
 July 1994.
\layout Bibliography
\bibitem {LL 96}

Peter Lee and Mark Leone, 
\begin_inset Quotes eld
\end_inset 

Optimizing ML with Run-Time Code Generation,
\begin_inset Quotes erd
\end_inset 

 In ACM SIGPLAN '96 Conference on Programming Language Design and Implementation
, Philadelphia, May 1996.
\layout Bibliography
\bibitem {Engler 96}

Dawson R.
 Engler, 
\begin_inset Quotes eld
\end_inset 

A Retargetable, Extensible, Very Fast Dynamic Code Generation System,
\begin_inset Quotes erd
\end_inset 

 SIGPLAN Conference on Programming Language Design and Implementation (PLDI
 '96), Philadelphia, PA, May 1996.
\layout Bibliography
\bibitem {KP 95}

Wayne Kelly and William Pugh, 
\begin_inset Quotes eld
\end_inset 

A Unifying Framework for Iteration Reordering Transformations,
\begin_inset Quotes erd
\end_inset 

 ???
\layout Bibliography
\bibitem {PS}

Massimiliano Poletto and Vivek Sarkar, 
\begin_inset Quotes eld
\end_inset 

Linear Scan Register Allocation,
\begin_inset Quotes erd
\end_inset 

 ACM ...
\layout Bibliography
\bibitem {Chaitin et al}

Chaitin, G.
 J., Auslander, M.
 A., Chandra, A.
 K., Cocke, J., Hopkins, M.
 E., and Markstein, P.
 W., 
\begin_inset Quotes eld
\end_inset 

Register allocation via coloring,
\begin_inset Quotes erd
\end_inset 

 Computer Languages 6, 47-57, 1981
\layout Bibliography
\bibitem {THS 98}

Traub, O., Holloway, G., and Smith, M.
 D., 
\begin_inset Quotes eld
\end_inset 

Quality and speed in linear-scan register allocation,
\begin_inset Quotes erd
\end_inset 

 Proc.
 ACM SIGPLAN '98 Conference on Programming Language Design and Implementation,
 1998
\layout Bibliography
\bibitem {Michie 68}

Michie, D., 
\begin_inset Quotes eld
\end_inset 

Memo Functions and Machine Learning,
\begin_inset Quotes erd
\end_inset 

 Nature 218, pp.
 19-22, 1968.
\layout Bibliography
\bibitem {Brown 00}

Brown, J., 
\begin_inset Quotes eld
\end_inset 

ARMphetamine: A Dynamically Recompiling ARM Emulator,
\begin_inset Quotes erd
\end_inset 

 Final-year dissertation, University of Cambridge, 2000.
\layout Bibliography
\bibitem [Appel 98]{Appel 98}

Appel, Andrew W., 
\begin_inset Quotes eld
\end_inset 

Modern Compiler Implementation in ML,
\begin_inset Quotes erd
\end_inset 

 Cambridge University Press, 1998
\the_end
@


1.4
log
@(no log message)
@
text
@d118 9
a126 1
Case study 1: Dynamo
d175 1
a175 1
\layout Subsection
d178 1
a178 1
\layout Subsection
d181 1
a181 1
\layout Subsection
d184 1
a184 1
\layout Subsection
d186 1
a186 1
Case study 2: Transmeta Crusoe
d207 1
a207 1
Limitations of a software-only approch
d210 4
a213 2
When writing a processor emulator in the style of ARMphetamine, one is not
 really modelling accurately what a processor actually does.
d219 2
a220 1
 an operating system designed for that architecture on a foreign machine.
d222 5
a226 4
 cycle-timing accuracy without compromising our ability to run that operating
 system, if only because different hardware processor implementations might
 also have different timings, so the operating system is written to perform
 timing by other means.
d229 1
a229 1
 whether code is being translated or run in a recompiled state.
d353 18
a370 2
 so that all code brought into a particular cache line is executed in the
 common case.
d398 3
a400 2
This requires a function to have no side effects, which may be a stumbling
 point.
d415 5
d444 26
a469 23
Many of these optimisations are difficult or impossible to perform in an
 emulation environment for a normal processor, since there is too little
 information available in most instruction-set architectures to know what
 is safe to do, and when to do it.
\layout Standard

The goal which all these optimisations together aim to achieve can be explained
 like this.
 Consider a computer programming language.
 Assume that this language can be executed in two ways, either by interpreting
 its statements one by one, or by compiling the whole program into machine
 code and executing that.
 Both methods of execution should produce the same result, but the compiled
 program will usually get there much faster.
 Why? Because we're removing redundant calculations.
\layout Standard

Now, consider what a dynamically-recompiling emulator does to achieve its
 speed.
 At the basic level the hosted program is run by software emulation of the
 original processor.
 `Hot' sections of code - actually data - are made into specialised subprograms
 which `process' themselves, so that the net result is the same as processing
d472 7
d484 1
a484 1
The Chameleon architecture is designed to enable as many of the previously-menti
d499 15
a513 1
Number of registers, simple encoding.
d519 5
a523 1
Wotcha.
d529 2
a530 1
Live things are good.
d534 4
d543 6
d635 9
d655 15
a669 1
 code, to achieve high speeds.
d697 5
a701 3
The solution is equivalent to the halting problem: say we can discover everythin
g a program can do from its start point, then we could determine whether
 there are any infinite loops in the code.
d705 2
a706 1
 Hence any code translation should be done at run time, `dynamically'.
d711 2
a712 2
 stronger issue than when dealing with a normal compiler, so special techniques
 must be developed which produce code quickly.
d716 1
a716 1
 The issue which causes the most problems though is that binary code was
d718 2
a719 1
 and issues such as the `boundary' cases of operations cause immense trouble.
d722 23
a744 1
Algorithm
d749 14
a762 1
 Firstly, we translate to an intermediate representation.
d764 2
a765 2
 is an important part of ARMphetamine, and forms the basis of the first
 iteration of the Chameleon instruction-set architecture
a866 45
\layout Subsubsection

Background
\layout Standard

This brings problems of its own, of course.
 The most obvious is that the translation itself must be 
\emph on 
fast
\emph default 
, which is not normally nearly so much of an issue with a traditional compiler.
 Other problems are how to know when to use the recompiler and when to `interpre
t' the code (emulate it in the normal way) and how to intermingle recompiled
 code and interpreted code.
 There is no `right' way of doing these things obviously, but this is the
 way I do it:
\layout Standard

Code on the guest machine is emulated normally.
 As this is done, profile information is gathered using a low-overhead technique
, of code which is in contiguous chunks.
 If one of these chunks (from a particular `entry point') is emulated more
 than a certain number of times, then that chunk is translated and cached
 so that native code can be run in place of it in the future.
\layout Standard

There are problem with such things as branches outside `known' code, but
 in those cases we can return control to the interpretive emulator, or insert
 hooks into the code to attach the previously-unknown code to if it gets
 recompiled at some point in the future.
 For the purposes of discussion, we will assume that each chunk of code
 contains only static internal (conditional) branches.
\layout Standard

Any memory access in a chunk can potentially cause a data abort (page fault)
 exception.
 Thus, it must be possible to recover the original state of the processor
 after such an exception.
 This limits the amount of optimisation which can be done.
 Code execution can potentially cause a prefetch abort exception, but the
 worst-case complexities of this can limited by disallowing chunks from
 spanning 4k (memory-management unit granularity) boundaries.
\layout Subsubsection

Form
d901 1
a901 1
\layout Subsection
d961 4
a964 1
 to essentially twiddling just these few bits.
d975 1
a975 1
 Trivially, when we encounter a conditional branch (say), we scan the ph2
d987 21
a1007 1
 them to be or not
d1015 1
@


1.3
log
@(no log message)
@
text
@d2 1
a2 1
\lyxformat 218
d37 46
a82 33
A virtual machine is a computer which is implemented in software and run
 on another computer.
 Virtual machines have become an integral part of computing today: Java,
 Transmeta's Crusoe processor, Intel's IA-64 with its built-in IA-32 hardware
 emulation layer, VMware for running two operating systems simultaneously
 on one PC, Connectix Virtual PC/Gamestation and Microsoft's .NET all utilise
 virtual machine technology.
 Virtual machines are useful for security: building safe sandboxes in which
 to run untrusted software, for the development of new hardware devices,
 as well as their traditional purpose of preserving software which would
 otherwise becomes unusable as the legacy hardware it runs on becomes obsolete.
\layout Standard

Computer processors have increased in complexity many times in recent years,
 though compiler technology has to some extent failed to keep up with the
 advances made.
 (...)
\layout Standard

I am going to investigate whether some of the problems encountered in virtual
 machine technology can be solved by novel processor design ideas, and whether
 extending this idea further can allow a self-hosted virtual machine to
 increase performance of its own applications as they are running, beyond
 the level which is possible in an ordinary compiler.
\layout Standard

The background of this research is a prototype dynamic recompilation system.
 I will start by explaining how this works, and why it isn't possible to
 do certain optimisations in that environment.
 I will then present a novel processor instruction-set architecture which
 will help relieve some of these problems, and I will explain how new optimisati
ons can be built on top of it.
 This prototype architecture shall be called ``Chameleon''.
d88 1
a88 1
Dynamic vs Profile-Driven Optimisation
d93 2
a94 2
 techniques to achieve any sort of reasonable performance (eg, Intel's Itanium
 architecture).
d104 3
a106 3
 it adds another layer of complexity to the build process.
 Problems with compilers in the past have led to much commercial software
 being shipped with optimisations turned off altogether (ref!).
d131 1
a131 1
 form, optimising then translating back to a different concrete ISA.
d133 2
a134 2
 that native binaries could actually improve in execution speed, sometimes
 by 20% or more.
d148 1
a148 1
 These fragments are `traces' from the original execution of the code -
d169 9
d199 1
a199 1
What is and isn't possible
d264 4
d277 4
d283 1
a283 1
Possible Optimisations
d339 1
a339 1
`Hot-cold' optimisation.
d343 6
a348 1
 (refs!)
d356 23
a378 1
 (refs!)
d396 5
a400 1
 .
d405 3
a407 1
ed into loops with better characteristics (refs!)
d435 15
d452 1
a452 1
Write a compiler by writing an interpreter
d455 4
a458 10
Run-time profile gathering and optimisation will allow one to write an interpret
er for a language, say -- or any other type of data-processing application
 for any data -- and produce a `compiler' for that language or application
 automatically.
 In essence this would be a system which takes the ``dynamic recompilation"
 approach on 
\emph on 
any
\emph default 
 data-dependent program.
d461 2
a462 5
Let me try to back this claim up.
 We make the assumption that a program which runs for a long time will process
 the same data, or the same ``sort" of data with commonly repeating sections.
 (...)
\layout Section
d464 1
a464 1
Chameleon Architecture
d467 8
a474 6
The Chameleon architecture is designed to enable as many of the previously-menti
oned optimisations as possible.
 It has a new instruction set designed specifically for ease of program
 analysis, but with a couple of concessions for code size.
 The main features at present are as follows:
\layout Itemize
d480 3
a482 1
 Code is organised in basic blocks, which are tagged with various meta-data
d487 9
a495 1
 All data is relocatable.
d503 2
a504 1
 Registers know if they are alive at a particular point in a program's execution.
d524 6
a529 2
It's undecided at present how code and data profiling hardware should behave,
 this is the subject of further research.
d531 58
a588 2
 be set aside for gathering profile data on running code, with as little
 overhead as possible.
d591 17
a607 1
Translating binary code from ARM to IA-32
d610 13
a622 4
I have written a compiler capable of translating ARM code to x86 (IA-32)
 code on-the-fly, as an acceleration mechanism for an ARM-based system emulator.
 This has given me much valuable insight.
 I will document the most-developed part of this below.
d630 8
a637 1
 iteration of the Chameleon instruction-set architecture.
d655 9
a663 1
 branch of compatible type to the target of the second branch.
d672 1
a672 1
 Find 'parent' nodes, start and finish times.
d729 1
a729 1
\layout Section
d731 2
a732 2
A Malleable Intermediate Representation
\layout Subsection
a736 22
Translating machine code from one architecture to another is a difficult
 problem.
 Though most processors perform many of the same operations at a basic level,
 the `interface' they present to the outside world can be very different,
 in terms of the bit-patterns or machine code which they use.
 These differences can be fairly major, such as whether fixed-length or
 variable-length instructions are used, or fairly subtle such as whether
 performing a particular operation causes side-effects in the processor's
 state.
\layout Standard

In a complete system emulation environment, it is not possible (or even
 desirable) to translate all of the available machine code to the host architect
ure.
 Even in the simpler case of translating, say, an entire application binary,
 without extra information which is not normally available it is not possible
 to translate it wholly into a native binary -- because the analysing program
 cannot know what is code and what is data.
 I will not go into the reasons more fully, but suffice to say that any
 code translation should be done dynamically, at `run-time'.
\layout Standard

d774 1
a774 1
\layout Subsection
d797 1
a797 1
\layout Subsection
d813 1
a813 1
Control Flow
d828 1
a828 1
 flags.
d857 6
a862 2
One approach (that used in ARMphetamine v1), is to save and restore flags
 individually, on-demand, in recompiled code.
d867 1
a867 1
 The code sequence used - not necessarily the best - took between 9 and
d889 8
d907 11
a917 2
Linear Scan is a fast register allocation technique developed by Poletto
 and Sarkar.
d923 6
a928 1
 technique similar in spirit to ``binpacking" [...refs...])
d1002 6
a1007 1
 Further analysis is available in [...ref...].
d1010 2
a1011 1
Any memory access instruction (load or store) in the original source code
d1013 10
a1022 3
 If this occurs in the middle of recompiled code, we must be able to recover
 the original processor state (there is no possibility of control returning
 to the original block at the point where the exception occurred).
d1027 1
a1027 1
 ARM registers may reside in IA-32 registers for the duration of a recompiled
d1029 10
a1038 4
 If an exception occurs, these must be spilled back to memory.
 This set of registers is only known as linear scan is in operation, so
 my solution is to generate code simultaneously with register allocation,
 up to the current start interval.
d1042 2
a1043 2
 generated cannot easily be spilled without rewriting that code, which would
 be wasteful and time-consuming.
d1045 1
a1045 1
 ``slots" where the operands should go.
d1047 17
a1063 4
 This approach has additional benefits where register non-orthogonalities
 are concerned, whereby if a particular register is needed for something
 and another register is available, the allocation state can be easily shifted
 around to make this so.
d1072 4
a1075 6
 Code is generated indirectly via an abstraction layer, which performs a
 similar task to a text-source assembler in choosing instruction variants
 for different operand types, but also knows about which instructions set
 up and destroy condition-code flags.
 A few simple substitutions are performed at this level for optimisation
 reasons.
d1103 151
@


1.2
log
@(no log message)
@
text
@d72 2
a73 2
Translating binary code from ARM to IA-32
\layout Standard
d75 1
a75 4
I have written a compiler capable of translating ARM code to x86 (IA-32)
 code on-the-fly, as an acceleration mechanism for an ARM-based system emulator.
 This has given me much valuable insight.
 I will document the most-developed part of this below.
d78 4
a81 6
Given a piece of ARM code, we perform several passes on it in order to translate
 it to IA-32 code.
 Firstly, we translate to an intermediate representation.
 I will describe this intermediate representation (ph2) below, since it
 is an important part of ARMphetamine, and forms the basis of the first
 iteration of the Chameleon instruction-set architecture.
d84 10
a93 3
The important things we need to do with the intermediate representation
 are extracting and optimising control flow, performing register allocation
 and doing IA-32 code generation.
d96 7
a102 79
Currently, the order of the passes required to do each of these is as follows:
\layout Itemize

Allocate constants.
 This means that ph2 variables which are loaded with constants need not
 have physical IA-32 registers attached to them later.
\layout Itemize

Optimise transitive branches.
 Rewrite the target of the first branch which points directly to a second
 branch of compatible type to the target of the second branch.
\layout Itemize

Cull unused nodes.
 As possibly generated by the previous step, or earlier in the ARM->ph2
 translation step.
\layout Itemize

Depth-first search.
 Find 'parent' nodes, start and finish times.
 Used by the strongly-connected component code later.
\layout Itemize

Shuffle commits.
 Move commits as early as possible, to relieve register pressure.
\layout Itemize

Find predecessors.
 This finds the inverse of the control flow graph.
\layout Itemize

Strongly-connected components.
 This finds strongly-connected components in the control flow graph.
\layout Itemize

Fix-up flags & predicates.
 Ensures that flags and control condition codes are stored when needed,
 and culls unnecessary flag calculation code.
\layout Itemize

Find spans.
 Find live variable ranges in ph2 code.
\layout Itemize

Source-destination alias.
 Where possible, alias the destination variable to the source variable of
 the three-address operations used by ph2.
 This is important for IA-32 code, where one of the source operands for
 calculations is usually overwritten by the result.
\layout Itemize

Re-run span finder.
 The aliases from the previous stage alter the variable liveness information.
\layout Itemize

Linear scan register allocation/code generation.
 Described below.
\layout Itemize

Insert load/store code.
 Cannot be known until after the previous stage has completed.
\layout Itemize

Finish allocation.
 The way register allocation works means some variables cannot be fully
 allocated until the end.
 This fixes them.
\layout Itemize

Flatten code.
 This takes abstract structure of IA-32 instructions, and flattens them
 into contiguous code.
\layout Standard

I will explain some of these stages further below, but first I will describe
 the ph2 intermediate language.
\layout Section

A Malleable Intermediate Representation
d105 1
a105 1
Background
d108 4
a111 9
Translating machine code from one architecture to another is a difficult
 problem.
 Though most processors perform many of the same operations at a basic level,
 the `interface' they present to the outside world can be very different,
 in terms of the bit-patterns or machine code which they use.
 These differences can be fairly major, such as whether fixed-length or
 variable-length instructions are used, or fairly subtle such as whether
 performing a particular operation causes side-effects in the processor's
 state.
d114 8
a121 9
In a complete system emulation environment, it is not possible (or even
 desirable) to translate all of the available machine code to the host architect
ure.
 Even in the simpler case of translating, say, an entire application binary,
 without extra information which is not normally available it is not possible
 to translate it wholly into a native binary -- because the analysing program
 cannot know what is code and what is data.
 I will not go into the reasons more fully, but suffice to say that any
 code translation should be done dynamically, at `run-time'.
d124 7
a130 2
This brings problems of its own, of course.
 The most obvious is that the translation itself must be 
d132 1
a132 1
fast
d134 6
a139 6
, which is not normally nearly so much of an issue with a traditional compiler.
 Other problems are how to know when to use the recompiler and when to `interpre
t' the code (emulate it in the normal way) and how to intermingle recompiled
 code and interpreted code.
 There is no `right' way of doing these things obviously, but this is the
 way I do it:
d142 6
a147 6
Code on the guest machine is emulated normally.
 As this is done, profile information is gathered using a low-overhead technique
, of code which is in contiguous chunks.
 If one of these chunks (from a particular `entry point') is emulated more
 than a certain number of times, then that chunk is translated and cached
 so that native code can be run in place of it in the future.
d150 4
a153 16
There are problem with such things as branches outside `known' code, but
 in those cases we can return control to the interpretive emulator, or insert
 hooks into the code to attach the previously-unknown code to if it gets
 recompiled at some point in the future.
 For the purposes of discussion, we will assume that each chunk of code
 contains only static internal (conditional) branches.
\layout Standard

Any memory access in a chunk can potentially cause a data abort (page fault)
 exception.
 Thus, it must be possible to recover the original state of the processor
 after such an exception.
 This limits the amount of optimisation which can be done.
 Code execution can potentially cause a prefetch abort exception, but the
 worst-case complexities of this can limited by disallowing chunks from
 spanning 4k (memory-management unit granularity) boundaries.
d156 1
a156 1
Form
d159 7
a165 2
The intermediate representation (ph2) is a three-address code, ie instructions
 are of the form:
d168 7
a174 14
add %7, %3, %4
\layout Standard

%3, %4 and %7 represent variables.
 %3 and %4 are source operands, %7 is the destination.
 Each of the operations available are simple (simpler than many RISC chips).
 ARM code in particular can be quite expressive, allowing immediate and
 shifted operands for data-processing instructions, predicated execution
 of instructions, etc.
 Complex ARM instructions are broken up into many ph2 instructions.
 Additionally, the ARM instruction encoding at a bit level is fairly involved,
 and ph2's encoding is very simple.
 This aids code analysis greatly, and has been very influential on the Chameleon
 architecture.
d177 1
a177 1
SSA
d180 3
a182 3
By convention, ph2 is always written in static single assignment (SSA) form.
 During the ARM->ph2 translation phase, this conversion is performed by
 performing
d184 1
a184 1
 register renaming
d186 65
a250 3
 in much the same way as a modern superscalar processor does.
 This form allows certain types of optimisation to be performed very quickly
 and easily (linear time vs quadratic time).
d253 1
a253 1
Control Flow
d256 4
a259 8
Ph2 instructions are grouped into blocks.
 There is no explicit branch instruction, but instead each block has a condition
 type, and pointers to a `true' block and a `false' block.
 It is not necessary to exactly mirror all of the condition-code flags which
 the source machine code used, since the host machine's native control-flow
 logic is used instead.
 Of course, flags must be calculated correctly before control returns to
 the interpretive emulator.
d262 8
a269 5
In more detail, on the ARM architecture, as on many others, flow control
 is achieved using condition codes, which are combinations of particular
 flags.
 For example, a piece of code might want to jump somewhere if one register
 is greater than or equal to another register:
d272 61
a332 1
cmp r4,r5
d335 10
a344 1
bge somewhere
d347 4
a350 5
The first instruction sets four flags, named Z (zero), N (negative), V (overflow
) and C (carry).
 The second tests a combination of those flags for the ``greater-than or
 equal'' condition, which is defined by ``N set and V set, or N clear and
 V clear", or more concisely as ``N==V".
d353 9
a361 9
The two architectures in question (ARM and IA-32) both employ flags for
 controlling flow like this, but have quite different semantics for setting
 and testing those flags.
 On the ARM for example, setting flags is optional for ALU operations, and
 any instruction may be conditionally executed.
 In IA-32, virtually all instructions unconditionally set (or in some cases
 corrupt) the processor flags.
 Nevertheless, we can still utilise the native processor's flag-calculation
 logic in recompiled code, although great care must be taken.
d364 11
a374 11
One approach (that used in ARMphetamine v1), is to save and restore flags
 individually, on-demand, in recompiled code.
 This was proven to work adequately, but produced bulky and slow code.
 The reason is mainly a shortcoming of IA-32, which makes it particularly
 difficult to alter individual bits corresponding to the S, Z and O flags
 in the flags register.
 The code sequence used - not necessarily the best - took between 9 and
 18 instructions to restore between one and four bits.
 When this was multiplied by the number of times such an operation was necessary
 in a chunk of code, this meant a fairly large proportion of code was dedicated
 to essentially twiddling just these few bits.
d377 6
a382 2
A better approach is to group flags forming a condition code together, and
 treat them as an atomic unit (IA-32 provides convenient set
d384 1
a384 1
xx
d386 1
a386 10
 instructions which allow you to do just that).
 This means that the state of the flags is not mirrored exactly in recompiled
 code, and the on-demand scheme can no longer be utilised.
 Trivially, when we encounter a conditional branch (say), we scan the ph2
 representation backwards (over block boundaries if necessary) until we
 find the instruction which last altered the flags corresponding to the
 relevant condition code.
 Then we insert an instruction which saves that condition code into a predicate
 buffer, and use that value rather than the condition flags when we decide
 whether to take the branch or not.
d389 4
a392 3
Dealing with an instruction set, either in the source or target architecture,
 which had no need for an implicit flag word would make translation much
 easier.
d395 45
a439 1
Modified Linear Scan
d442 5
a446 8
Linear Scan is a fast register allocation technique developed by Poletto
 and Sarkar.
 Unfortunately in its basic form it is inconvenient for use in a dynamic
 recompilation system since information it requires whilst working is needed
 during other stages of code generation.
 First I'll explain how it normally works, then I'll describe my modification.
 (An earlier version of ARMphetamine used an ad-hoc register allocation
 technique similar in spirit to ``binpacking" [...refs...])
d449 1
a449 4
The algorithm works as follows.
 It is assumed that program instructions can be laid out in some order,
 for instance the order in which the intermediate representation is laid
 out in memory or the order arising from a depth-first search.
d452 4
a455 22
The intermediate representation is scanned and from it a set of 
\emph on 
live intervals
\emph default 
 for variables is inferred.
 Two live intervals interfere if they overlap.
 Given 
\emph on 
R
\emph default 
 available registers and a list of live intervals, the linear scan algorithm
 must allocate registers to as many intervals as possible, such that no
 two overlapping intervals are assigned the same register.
 If 
\begin_inset Formula \( n>R \)
\end_inset 

 live intervals overlap at any point, at least 
\begin_inset Formula \( n-R \)
\end_inset 

 of them must reside in memory.
d458 6
a463 5
The number of overlapping intervals only changes at the start and end of
 an interval.
 Live intervals are stored in an list sorted by increasing start point,
 so the algorithm can progress quickly by skipping from one start interval
 to the next.
d466 3
a468 15
At each step, the algorithm maintains a list of 
\emph on 
active
\emph default 
 live intervals, which have been assigned to registers.
 This list is kept in order of increasing end point.
 For each new interval, the algorithm scans 
\emph on 
active
\emph default 
 and removes any `expired' intervals which are no longer pertinent to the
 current position in the source program, leaving such interval's registers
 free for reallocation.
 Since this list is stored in increasing end-point order, scanning can stop
 when an end-point greater than the current new start point has been encountered.
d471 2
a472 3
The worst case scenario, where the active list has length 
\begin_inset Formula \( R \)
\end_inset 
d474 4
a477 15
 and no intervals have been expired, is resolved by choosing one of the
 intervals from 
\emph on 
active
\emph default 
 or the new start interval.
 As a heuristic, the interval which ends furthest from the current point
 is spilled to memory.
 This information can be found quickly since 
\emph on 
active
\emph default 
 is sorted by increasing end point.
 Further analysis is available in [...ref...].
\layout Standard
d479 4
a482 8
Any memory access instruction (load or store) in the original source code
 can cause a data abort exception.
 If this occurs in the middle of recompiled code, we must be able to recover
 the original processor state (there is no possibility of control returning
 to the original block at the point where the exception occurred).
 Up to 
\begin_inset Formula \( R \)
\end_inset 
d484 4
a487 7
 ARM registers may reside in IA-32 registers for the duration of a recompiled
 block.
 If an exception occurs, these must be spilled back to memory.
 This set of registers is only known as linear scan is in operation, so
 my solution is to generate code simultaneously with register allocation,
 up to the current start interval.
\layout Standard
d489 4
a492 11
The problem then is that an interval used by code which has already been
 generated cannot easily be spilled without rewriting that code, which would
 be wasteful and time-consuming.
 The solution to this is to generate code in an abstracted form, with mutable
 ``slots" where the operands should go.
 These slots are only finalised when the linear scan algorithm has completed.
 This approach has additional benefits where register non-orthogonalities
 are concerned, whereby if a particular register is needed for something
 and another register is available, the allocation state can be easily shifted
 around to make this so.
\layout Section
d494 3
a496 2
Instruction Selection
\layout Standard
d498 3
a500 10
We generate code by simply choosing IA-32 instructions which correspond
 to ph2 instructions one at a time (ARMphetamine v1 used a complicated tree-base
d matching method).
 Code is generated indirectly via an abstraction layer, which performs a
 similar task to a text-source assembler in choosing instruction variants
 for different operand types, but also knows about which instructions set
 up and destroy condition-code flags.
 A few simple substitutions are performed at this level for optimisation
 reasons.
\layout Section
d502 3
a504 2
Flattening Code
\layout Standard
d506 3
a508 8
Given a graph of basic blocks, there are any number of ways of laying out
 those blocks into a linear instruction stream.
 My algorithm is fairly ad-hoc, and as yet has not been subjected to rigourous
 testing.
 A priority queue is used to decide which block to output next.
 The `true' and `false' blocks for each block by default are added to this
 queue after that block has been generated.
 Their priorities are increased if they are:
d511 2
a512 1
Part of the same strongly-connected component as the current block
d515 5
a519 1
For the true block, if there is no false block
d522 3
a524 3
In the future, a weighting might be added depending on how often branches
 were taken/not taken in the profiling of the original code before translation.
\layout Standard
d526 3
a528 3
The rationale being that spatially-adjacent code accesses are more likely
 to be accessed close together in time.
\layout Section
d530 3
a532 2
Dynamic Optimisation
\layout Subsection
d534 5
a538 2
Dynamic vs Profile-Driven Optimisation
\layout Standard
d540 3
a542 4
There has been much focus recently on profile-driven optimisation techniques.
 In fact, certain new processor designs are practically dependent on such
 techniques to achieve any sort of reasonable performance (eg, Intel's Itanium
 architecture).
d545 3
a547 11
There are two main problems with the profile-driven approach.
 The first is that a program must be run with a set of `typical' data at
 compile time, and such data is not always available, or the program will
 be run on such diverse data that it is not possible to pinpoint a particular
 data set to optimise on.
 The second problem is more pragmatic, in that it is difficult to persuade
 developers to develop and ship code using profile-driven compilation, since
 it adds another layer of complexity to the build process.
 Problems with compilers in the past have led to much commercial software
 being shipped with optimisations turned off altogether (ref!).
\layout Standard
d549 1
a549 7
Dynamic optimisation solves each of these problems.
 In the latter case, developers do not need to perform extra profile-gathering
 and optimisation passes.
 In the former case, since code can adapt to whatever data it is running
 on at the time, there is no need to create contrived `typical' data sets.
 Even data which changes in character as a program is running can be handled,
 as the program may adapt to those changes.
d552 12
a563 1
Case study 1: Dynamo
d566 9
a574 4
One of the best-documented attempts at dynamic optimisation is Hewlett Packard's
 Dynamo.
 The aim of their research is to speed up native HP/UX binaries running
 on PA-RISC powered workstations.
d577 11
a587 8
This was not the original aim of the project.
 They too were investigating how binary translation can be used to execute
 non-native binaries at native speeds.
 That is, reading in code from a particular ISA, translating to an internal
 form, optimising then translating back to a different concrete ISA.
 For ease of implementation, they chose to target the same ISA, and found
 that native binaries could actually improve in execution speed, sometimes
 by 20% or more.
d590 6
a595 16
Dynamo executes entirely in user space on unmodified hardware.
 Much like ARMphetamine, it starts out by always 
\emph on 
emulating
\emph default 
 the code it's running.
 It profiles code as it runs it, building up a 
\emph on 
fragment cache
\emph default 
 of sections of code which are executed frequently.
 These fragments are `traces' from the original execution of the code -
 that is, they linearise complicated control flow, turning it into straight-line
 code.
 This can be handled more efficiently by the CPU's prefetch hardware and
 instruction cache.
d598 6
a603 6
The other way Dynamo gains an advantage over statically-optimised code is
 by using its own lightweight optimiser.
 Since optimisation is done at run-time, optimisations themselves must be
 done very quickly.
 The optimisations attempted include optimisations across procedure call/return
 boundaries and indirect branches or virtual function calls.
d606 8
a613 4
Since Dynamo operates on native binaries, if at any point it decides that
 too much time is being spent in re-optimisation, it can ``bail out'' and
 return control to the native processor.
 This obviously isn't possible if non-native binaries are being used.
d616 5
a620 1
Case study 2: Transmeta Crusoe
d623 1
a623 7
Transmeta's Crusoe processor takes steps beyond Dynamo in several important
 respects.
 It adds support for exceptions and speculation (shadow registers), optimization
 of memory operations (alias hardware), and self-modifying code (a ``translated"
 bit in the MMU).
 And of course it works on non-native binaries, executing IA-32 code on
 a VLIW processor core.
d626 11
a636 7
Crusoe's method of operation is otherwise similar to Dynamo's.
 It starts out emulating code, then for frequently executed sections it
 does a `rough' translation to its own native (VLIW) code.
 These translations are cached and further optimised over time the more
 they are executed.
 By this method the designers claim that performance comparable to a modern
 hardware IA-32 implementation is obtained.
d639 1
a639 1
What is and isn't possible
d642 3
a644 3
When writing a processor emulator in the style of ARMphetamine, one is not
 really modelling accurately what a processor actually does.
 In fact, it's more like designing an 
d646 1
a646 1
implementation
d648 6
a653 10
 of an instruction-set architecture, perhaps for the purpose of running
 an operating system designed for that architecture on a foreign machine.
 In many cases, we can fairly safely take short-cuts on issues such as perfect
 cycle-timing accuracy without compromising our ability to run that operating
 system, if only because different hardware processor implementations might
 also have different timings, so the operating system is written to perform
 timing by other means.
 We still encounter problems when attempting to run system hardware timers,
 used for dealing with I/O etc., which have to be produced in a timely manner
 whether code is being translated or run in a recompiled state.
d656 8
a663 19
What we must not do is let the state of the virtual machine diverge from
 the state of the equivalent real machine.
 For example, if a leaf subroutine affects the processor flags or a `scratch'
 register, we cannot optimise that calculation away, even if the context
 in which that leaf subroutine is called is insensitive to the result of
 that calculation.
 We cannot be sure that the subroutine is not called from elsewhere in such
 a way that the calculation is vital.
 This seems obvious, but severely limits the kind of optimisations we can
 perform.
 For example, say we were translating code from some hypothetical CISC processor
 with eight general-purpose registers available for data processing.
 In many cases these are not sufficient to hold all the temporary values
 necessary to perform a calculation, so some of them must instead be stored
 in memory locations, say on a stack.
 If we are translating this code to some RISC architecture with 32 available
 registers, it seems desirable to assign some of the stack locations to
 registers instead, to make better use of the resources available.
 The anticipated problems with this are as follows:
d666 5
a670 14
We are looking at the code through a potentially very small `window'.
 This leads to a bad case of the `pointer aliasing' problem, whereby we
 can't tell if loads or stores to particular memory locations will affect
 loads or stores from other memory locations.
 If we factor out a store and a load from what we assume to be the stack,
 and place the value in a register for the duration, an intermediate load
 or store which should affect that value, will not.
 Transmeta used additional hardware to help work around this problem, which
 is not available if we take a software-only approach.
 Note it doesn't matter if such register reallocation would work 
\emph on 
most of the time
\emph default 
 -- if it will sometimes fail we simply can't do it.
d673 1
a673 7
Similarly, we can't tell (or at least, we probably can't tell) if a particular
 load or store points at some volatile memory location, such as I/O space
 or a memory location which is touched by code running under some sort of
 interrupt.
 Also similarly, most of the time we could probably make assumptions, but
 the rare cases in which we are wrong would lead to unacceptable program
 behaviour.
d676 2
a677 8
Exceptions are another problem.
 Exceptions can be divided into two categories, those which occur asynchronously
, where the program counter location is of little consequence (eg, keyboard
 interrupts), and those which occur synchronously, such as page faults.
 In the latter case, it's vital that the original state of the processor
 can be recovered, so we can find exactly where the fault occurred and continue
 from that point.
\layout Subsection
d679 5
a683 1
Possible Optimisations
d686 9
a694 4
Transmeta's Crusoe processor, as mentioned, overcomes some of these problems
 using special hardware.
 However, the optimisations they can perform are still limited in scope
 by the necessity of using IA-32 as their source ISA.
d697 11
a707 8
It's worth taking a step back and looking at what might be possible if we
 weren't constrained by modelling an existing architecture, but instead
 designed a new ISA specifically tailored for the demands of dynamic optimisatio
n.
 In particular, we would like code which is exceptionally easy to analyse.
 In addition, we should retain certain information in the instruction stream
 regarding loops, functions and variable liveness which are not normally
 present in binary images.
d710 16
a725 36
As an idea of the kind of thing we'd hope would be possible with such an
 ISA, here are a few sample optimisations which could be performed.
\layout Itemize

Shared-library stub bypassing.
 This should be an obvious, easy optimisation.
 Dynamic-linked libraries normally link references to an external object
 through some sort of jump table, adding an extra level of call overhead
 per function call in the shared library.
 
\layout Itemize

Inlining of `hot' functions.
 Going one step beyond removing a layer of call overhead to a function,
 the function might actually be inserted directly into the code in the necessary
 places.
 There are obviously trade-offs with code expansion vs any speedup gained,
 except perhaps in the case where very small leaf functions (eg, accessor
 functions in C++) take up less space than the original function-call sequence,
 in which case inlining is a no-brainer.
\layout Itemize

Re-optimising around inlined functions:
\begin_deeper 
\layout Itemize

Once we've inlined a function, we can remove any code which, say, pushes
 arguments onto the stack only to pull them off again immediately.
 Any code dealing with stack frames can be removed.
\layout Itemize

Re-running common subexpression elimination, dead code removal.
 An inline function will often perform surplus work which might be unnecessary
 in a particular calling context
\end_deeper 
\layout Itemize
d727 4
a730 6
`Hot-cold' optimisation.
 Given continuously-gathered profile information, we can rearrange functions
 so that all code brought into a particular cache line is executed in the
 common case.
 (refs!)
\layout Itemize
d732 2
a733 7
Partial evaluation.
 Continuously-gathered data profiling might enable specialised versions
 of functions to be generated, with some or all arguments replaced by constants.
 Run-time constants enable further optimisations such as loop unrolling
 to be performed.
 (refs!)
\layout Itemize
d735 9
a743 4
Data and code prefetching.
 Continuously-gathered profile information might enable prefetch instructions
 to pull data from main memory into caches in a timely fashion
\layout Itemize
d745 4
a748 4
Instruction scheduling.
 If execution unit utilisation is low for a piece of code, instruction schedulin
g might be re-run for that code
\begin_float footnote 
d751 20
a770 6
This requires a simulation of the new architecture detailed enough to take
 into account multiple issue execution units and cycle accuracy.
 Given constraints on resources available, this may be impossible to achieve
\end_float 
 .
\layout Itemize
d772 1
a772 3
Loop transformations.
 If nested loops turn out to cause poor cache utilisation, they can be transform
ed into loops with better characteristics (refs!)
d775 5
a779 4
Many of these optimisations are difficult or impossible to perform in an
 emulation environment for a normal processor, since there is too little
 information available in most instruction-set architectures to know what
 is safe to do, and when to do it.
d782 15
a796 9
The goal which all these optimisations together aim to achieve can be explained
 like this.
 Consider a computer programming language.
 Assume that this language can be executed in two ways, either by interpreting
 its statements one by one, or by compiling the whole program into machine
 code and executing that.
 Both methods of execution should produce the same result, but the compiled
 program will usually get there much faster.
 Why? Because we're removing redundant calculations.
d799 3
a801 9
Now, consider what a dynamically-recompiling emulator does to achieve its
 speed.
 At the basic level the hosted program is run by software emulation of the
 original processor.
 `Hot' sections of code - actually data - are made into specialised subprograms
 which `process' themselves, so that the net result is the same as processing
 the data in the original, slow way.
 Except, of course, that the desired result is obtained much faster.
\layout Subsection
d803 9
a811 9
Write a compiler by writing an interpreter
\layout Standard

Run-time profile gathering and optimisation will allow one to write an interpret
er for a language, say -- or any other type of data-processing application
 for any data -- and produce a `compiler' for that language or application
 automatically.
 In essence this would be a system which takes the ``dynamic recompilation"
 approach on 
d813 1
a813 1
any
d815 2
a816 1
 data-dependent program.
d819 8
a826 5
Let me try to back this claim up.
 We make the assumption that a program which runs for a long time will process
 the same data, or the same ``sort" of data with commonly repeating sections.
 (...)
\layout Section
d828 6
a833 1
Chameleon Architecture
d836 11
a846 6
The Chameleon architecture is designed to enable as many of the previously-menti
oned optimisations as possible.
 It has a new instruction set designed specifically for ease of program
 analysis, but with a couple of concessions for code size.
 The main features at present are as follows:
\layout Itemize
d848 2
a849 2
Variable liveness information encoded in the instruction stream.
\layout Itemize
d851 10
a860 3
Novel control flow.
 Code is organised in basic blocks, which are tagged with various meta-data
\layout Itemize
d862 2
a863 4
No addresses.
 Other basic blocks are referred to by references.
 All data is relocatable.
\layout Itemize
d865 8
a872 2
No special condition code registers.
 Like Alpha, normal registers are used to hold condition codes.
d875 1
a875 5
32-bit registers with additional tags.
 Registers know if they are alive at a particular point in a program's execution.
 There are per-register carry and overflow flags to deal with multi-precision
 arithmetic.
 This allows one `type' to be used for dataflow analysis throughout, hopefully.
d878 1
a878 5
Multi-register load & store instructions.
 Modelled after the ARM, but only allowing one contiguous range of registers
 to be stored.
 Reduces stack frame manipulation code to one or two instructions, without
 loss of generality.
d881 2
a882 5
Large number of (64) registers, most of which are general-purpose.
\layout Itemize

Four sets of fully-shadowing registers for cheap exception handling, and
 to allow code analysis to take place without altering the processor's state
d885 2
a886 5
It's undecided at present how code and data profiling hardware should behave,
 this is the subject of further research.
 It's anticipated that either special cache or an area of main memory might
 be set aside for gathering profile data on running code, with as little
 overhead as possible.
@


1.1
log
@Initial revision
@
text
@d2 1
a2 1
\lyxformat 2.16
d4 1
a4 1
\language default
d41 1
a41 1
 emulation layer, VMware for running two operating systems simulaneously
d69 1
a69 1
 This prototype architecture shall be called ``Chameleon".
d343 2
a344 2
 find the instruction which last altered the flags correponding to the relevent
 condition code.
d515 30
d610 1
a610 1
 does a `rough' translation to its own native code.
d613 2
a614 2
 By this method the designers claim that performance comparable to hardware
 IA-32 implementation is obtained.
d627 1
a627 1
 an operating system designed for that architecuture on a foreign machine.
a784 4
\begin_float footnote 
\layout Standard

\end_float 
@
