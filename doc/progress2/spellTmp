<html><head><title>The Design of ARMphetamine 2</title></head>
<body text="#ffffff" bgcolor="#000080" link="#00ffff" vlink="#ffffff" alink="#00ffff">

<center><h1>The Design of ARMphetamine 2</h1></center>
<p align=center>Julian Brown, 27 March 2001. Modified 25 April 2001.

<h3>Background</h3>

<p>A virtual machine is a computer which is implemented in software and run on another computer. Virtual machines have become an integral part of computing today: Java, Transmeta's Crusoe processor, Intel's IA-64 with its built-in IA-32 hardware emulation layer, VMware for running two operating systems simulateously on one PC, Connectix Virtual PC/Gamestation and Microsoft's .NET all utilise virtual machine technology. Virtual machines are useful for security: building safe sandboxes in which to run untrusted software, for the development of new hardware devices, as well as their traditional purpose of preserving software which would otherwise becomes unusable as the legacy hardware it runs on becomes obsolete.

<p>Computer processors have increased in complexity many times in recent years, though compiler technology has to some extent failed to keep up with the advances made. (...)

<p>I am going to investigate whether some of the problems encountered in virtual machine technology can be solved by novel processor design ideas, and whether extending this idea further can allow a self-hosted virtual machine to increase performance of its own applications as they are running, beyond the level which is possible in an ordinary compiler.

<p>The background of this research is a prototype dynamic recompilation system. I will start by explaining how this works, and why it isn't possible to do certain optimisations in that environment. I will then present a novel processor instruction-set architecture which will help relieve some of these problems, and I will explain how new optimisations can be built on top of it. This prototype architecture shall be called "Chameleon".

<h3>ARMphetamine: translating ARM machine code to Intel x86 code</h3>

<p>I have written a compiler capable of translating ARM code to x86 (IA-32) code on-the-fly, as an acceleration mechanism for an ARM-based system emulator. This has given me much valuable insight. I will document the most-developed part of this below.

<p>Given a piece of ARM code, we perform several passes on it in order to translate it to IA-32 code. Firstly, we translate to an intermediate representation. I will describe this intermediate representation (ph2) below, since it is an important part of ARMphetamine, and forms the basis of the first iteration of the Chameleon instruction-set architecture.

<p>The important things we need to do with the intermediate representation are extracting and optimising control flow, performing register allocation and doing IA-32 code generation.

<p>Currently, the order of the passes required to do each of these is as follows:

<ol>
<li>Allocate constants. This means that ph2 variables which are loaded with constants need not have physical IA-32 registers attached to them later.
<li>Optimise transitive branches. Rewrite the target of the first branch which points directly to a second branch of compatible type to the target of the second branch.
<li>Cull unused nodes. As possibly generated by the previous step, or earlier in the ARM->ph2 translation step.
<li>Depth-first search. Find 'parent' nodes, start and finish times. Used by the strongly-connected component code later.
<li>Shuffle commits. Move commits as early as possible, to relieve register pressure.
<li>Find predecessors. This finds the inverse of the control flow graph.
<li>Strongly-connected components. This finds strongly-connected components in the control flow graph.
<li>Fix-up flags &amp; predicates. Ensures that flags and control condition codes are stored when needed, and culls unnecessary flag calculation code.
<li>Find spans. Find live variable ranges in ph2 code.
<li>Source-destination alias. Where possible, alias the destination variable to the source variable of the three-address operations used by ph2. This is important for IA-32 code, where one of the source operands for calculations is usually overwritten by the result.
<li>Re-run span finder. The aliases from the previous stage alter the variable liveness information.
<li>Linear scan register allocation/code generation. Described below.
<li>Insert load/store code. Cannot be known until after the previous stage has completed.
<li>Finish allocation. The way register allocation works means some variables cannot be fully allocated until the end. This fixes them.
<li>Flatten code. This takes abstract structure of IA-32 instructions, and flattens them into contiguous code.
</ol>

<p>I will explain some of these stages further below, but first I will describe the ph2 intermediate language.

<h3>A Malleable Intermediate Representation: ph2</h3>

<h4>Background</h4>

<p>Translating machine code from one architecture to another is a difficult problem. Though most processors perform many of the same operations at a basic level, the 'interface' they present to the outside world can be very different, in terms of the bit-patterns or machine code which they use. These differences can be fairly major, such as whether fixed-length or variable-length instructions are used, or fairly subtle such as whether performing a particular operation causes side-effects in the processor's state.

<p>In a complete system emulation environment, it is not possible (or even desirable) to translate all of the available machine code to the host architecture. Even in the simpler case of translating, say, an entire application binary, without extra information which is not normally available it is not possible to translate it wholly into a native binary -- because the analysing program cannot know what is code and what is data. I will not go into the reasons more fully, but suffice to say that any code translation should be done dynamically, at 'run-time'.

<p>This brings problems of its own, of course. The most obvious is that the translation itself must be <i>fast</i>, which is not normally nearly so much of an issue with a traditional compiler. Other problems are how to know when to use the recompiler and when to 'interpret' the code (emulate it in the normal way) and how to intermingle recompiled code and interpreted code. There is no 'right' way of doing these things obviously, but this is the way we do it:

<p>Code on the guest machine is emulated normally. As this is done, profile information is gathered using a low-overhead technique, of code which is in contiguous chunks. If one of these chunks (from a particular 'entry point') is emulated more than a certain number of times, then that chunk is translated and cached so that native code can be run in place of it in the future.

<p>There are problem with such things as branches outside 'known' code, but in those cases we can return control to the interpretive emulator, or insert hooks into the code to attach the previously-unknown code to if it gets recompiled at some point in the future. For the purposes of discussion, we will assume that each chunk of code contains only static internal (conditional) branches.

<p>Any memory access in a chunk can potentially cause a data abort (page fault) exception. Thus, it must be possible to recover the original state of the processor after such an exception. This limits the amount of optimisation which can be done. Code execution can potentially cause a prefetch abort exception, but the worst-case complexities of this can limited by disallowing chunks from spanning 4k (memory-management unit granularity) boundaries.

<h4>Form</h4>

<p>The intermediate representation (ph2) is a three-address code, ie instructions are of the form:

<pre>
        add %7, %3, %4
</pre>

<p>%7, %3 and %4 represent variables. %3 and %4 are source operands, %7 is the destination. Each of the operations available are simple (simpler than many RISC chips). ARM code in particular can be quite complex, allowing immediate and shifted operands for data-processing instructions, predicated execution of instructions, etc. Complex ARM instructions are broken up into many ph2 instructions. Additionally, the ARM instruction encoding at a bit level is fairly complex, and ph2's encoding is very simple. This aids code analysis greatly.

<h4>SSA</h4>

<p>By convention, ph2 is always written in static single assignment (SSA) form. During the ARM->ph2 translation phase, this conversion is performed by performing <b>register renaming</b> in much the same way as a modern superscalar processor does. This form allows certain types of optimisation to be performed very quickly and easily (linear time vs quadratic time).

<h4>Control Flow</h4>

<p>ph2 instructions are grouped into blocks. There is no explicit branch instruction, but instead each block has a condition type, and pointers to a 'true' block and a 'false' block. It is not necessary to exactly mirror all of the condition-code flags which the source machine code used, since the host machine's native control-flow logic is used instead. Of course, flags must be calculated correctly before control returns to the interpretive emulator.

<p>In more detail, on the ARM (as in many) architecture, flow control is achieved using condition codes, which are combinations of particular flags. For example, a piece of code might want to jump somewhere if one register is greater than or equal to another register:
<pre>
        <b>cmp</b> r4,r5
        <b>bge</b> somewhere
</pre>
<p>The first instruction sets four flags, named Z (zero), N (negative), V (overflow) and C (carry). The second tests a combination of those flags for the "greater-than or equal" condition, which is defined by "N set and V set, or N clear and V clear", or more concisely as "N==V".

<p>The two architectures in question (ARM and IA-32) both employ flags for controlling flow like this, but have quite different semantics for setting and testing those flags. On the ARM for example, setting flags is optional for ALU operations, and any instruction may be conditionally executed. In IA-32, virtually all instructions unconditionally set (or corrupt) the processor flags. Nevertheless, we can still utilise the native processor's flag-calculation logic in recompiled code, although great care must be taken.

<p>One approach (that used in ARMphetamine v1), is to save and restore flags individually, on-demand, in recompiled code. This was proven to work adequately, but produced bulky and slow code. The reason is mainly a shortcoming of IA-32, which makes it particularly difficult to alter individual bits corresponding to the S, Z and O flags in the flags register. The code sequence used - not necessarily the best - took between 9 and 18 instructions to restore between one and four bits. When this was multiplied by the number of times such an operation was necessary in a chunk of code, this meant a fairly large proportion of code was dedicated to essentially twiddling just a couple of bits.

<p>A better approach is to group flags forming a condition code together, and treat them as an atomic unit (IA-32 provides convenient <tt>set<i>xx</i></tt> instruction which allow you to do just that). This means that the state of the flags is not mirrored exactly in recompiled code, and the on-demand scheme can no longer be utilised. Trivially, when we encounter a conditional branch (say), we scan the ph2 representation backwards (over block boundaries if necessary) until we find the instruction which last altered the flags correponding to the relevent condition code. Then we insert an instruction which saves that condition code into a predicate buffer, and use that value rather than the condition flags when we decide whether to take the branch or not.

<h3>Modified Linear Scan</h3>

<p>Linear Scan is a fast register allocation technique developed by Poletto and Sarkar. Unfortunately in its basic form it is inconvenient for use in a dynamic recompilation system since information it requires whilst working is needed during other stages of code generation. First I'll explain how it normally works, then I'll describe my modification. (An earlier version of ARMphetamine used an ad-hoc register allocation technique similar in spirit to "binpacking" [...refs...])

<p>The algorithm works as follows. It is assumed that program instructions can be laid out in some order, for instance the order in which the intermediate representation is laid out in memory or the order arising from a depth-first search.

<p>The intermediate representation is scanned and from it a set of <i>live intervals</i> for variables is inferred. Two live intervals interfere if they overlap. Given <i>R</i> available registers and a list of live intervals, the linear scan algorithm must allocate registers to as many intervals as possible, such that no two overlapping intervals are assigned the same register. If <i>n</i> &gt; <i>R</i> live intervals overlap at any point, at least <i>n</i> - <i>R</i> of them must reside in memory.

<p>The number of overlapping intervals only changes at the start and end of an interval. Live intervals are stored in an list sorted by increasing start point, so the algorithm can progress quickly by skipping from one start interval to the next.

<p>At each step, the algorithm maintains a list of <i>active</i> live intervals, which have been assigned to registers. This list is kept in order of increasing <i>end</i> point. For each new interval, the algorithm scans <i>active</i> and removes any 'expired' intervals which are no longer pertinent to the current position in the source program, leaving such interval's registers free for reallocation. Since this list is stored in increasing end-point order, scanning can stop when an end-point greater than the current new start point has been encountered.

<p>The worst case scenario, where the active list has length <i>R</i> and no intervals have been expired, is resolved by choosing one of the intervals from <i>active</i> or the new start interval. As a heuristic, the interval which ends <i>furthest</i> from the current point is spilled to memory. This information can be found quickly since <i>active</i> is sorted by increasing end point. Further analysis is available in [...ref...].

<p>Any memory access instruction (load or store) in the original source code can cause a data abort exception. If this occurs in the middle of recompiled code, we must be able to recover the original processor state (there is no possibility of control returning to the original block at the point where the exception occurred). Up to <i>R</i> ARM registers may reside in IA-32 registers for the duration of a recompiled block. If an exception occurs, these must be spilled back to memory. This set of registers is only known as linear scan is in operation, so my solution is to generate code simultaneously with register allocation, up to the current start interval.

<p>The problem then is that an interval used by code which has already been generated cannot easily be spilled without rewriting that code, which would be prohibitively complex. The solution to this is to generate code in an abstracted form, with mutable "slots" where the operands should go. These slots are only 'finalised' when the linear scan algorithm has completed. This approach has additional benefits where register non-orthogonalities are concerned, whereby if a particular register is needed for something and another register is available, the allocation state can be easily shifted around to make this good.

<h3>Instruction Selection</h3>

<p>We generate code by simply choosing IA-32 instructions which correspond to ph2 instructions one at a time (ARMphetamine v1 used a complicated tree-based matching method). Code is generated indirectly via an abstraction layer, which performs a similar task to a text-source assembler in choosing instruction variants for different operand types, but also knows about which instructions set up and destroy condition-code flags. A few simple substitutions are performed at this level for optimisation reasons.

<h3>Flattening Code</h3>

<p>Given a graph of basic blocks, there are any number of ways of laying out those blocks into a linear instruction stream. My algorithm is fairly ad-hoc, and as yet has not been subjected to rigourous testing. A priority queue is used to decide which block to output next. The 'true' and 'false' blocks for each block by default are added to this queue after that block has been generated. Their priorities are increased if they are:

<ul>
<li>Part of the same strongly-connected component as the current block
<li>For the true block, if there is no false block
<li>In the future, a weighting might be added depending on how often branches were taken/not taken in the profiling of the original code before translation.
</ul>

<p>The rationale being that spatially-adjacent code accesses are more likely to be accessed temporally-adjacently.

<h3>Beyond dynamic code translation: Dynamic optimisation</h3>

<h4>What is and isn't possible</h4>

<p>When writing a processor emulator like this, one is not really modelling accurately what a processor actually does. In fact, it's more like designing an <i>implementation</i> of an instruction-set architecture, perhaps for the purpose of running an operating system designed for that architecuture on a foreign machine. In many cases, we can fairly safely take short-cuts on issues such as perfect cycle-timing accuracy without compromising our ability to run that operating system, if only because different hardware processor implementations might also have different timings, so the operating system is written to perform timing by other means.

<p>What we cannot do is let the state of the virtual machine diverge from the state of the equivalent real machine. For example, if a leaf subroutine affects the processor flags or a 'scratch' register, we cannot optimise that calculation away, even if the context in which that leaf subroutine is called is insensitive to the result of that calculation. We cannot be sure that the subroutine is not called from elsewhere in such a way that the calculation is vital. The amount of information available regarding variable liveness is not sufficient in existing processor archictures.

<h4>Possible Optimisations</h4>

<p>It's worth taking a step back and looking at what might be possible if we weren't constrained by modelling an existing architecture. The potential for optimising programs on-the-fly is huge. Some of the things which are possible:

<ul>
<li>Shared-library stub bypassing. This should be an obvious, easy optimisation. Dynamic-linked libraries normally link references to an external object through some sort of jump table, adding an extra level of call overhead per function call in the shared library.
<li>Inlining of 'hot' functions. Going one step beyond removing a layer of call overhead to a function, the function might actually be inserted directly into the code in the necessary places. There are obviously trade-offs with code expansion vs any speedup gained, except perhaps in the case where very small leaf functions (eg, accessor functions in C++) take up less space than the original function-call sequence, in which case inlining is a no-brainer.

<li>Re-optimising around inlined functions:
  <ul>
  <li>Once we've inlined a function, we can remove any code which, say, pushes arguments onto the stack only to pull them off again immediately. Any code dealing with stack frames can be removed.
  <li>Re-running common subexpression elimination, dead code removal. An inline function will often perform surplus work which might be unnecessary in a particular calling context
</ul>

<li>'Hot-cold' optimisation. Given continuously-gathered profile information, we can rearrange functions so that all code brought into a particular cache line is executed in the common case.
<li>Partial evaluation. Continuously-gathered data profiling might enable specialised versions of functions to be generated, with some or all arguments replaced by constants. Run-time constants enable further optimisations such as loop unrolling to be performed.
<li>Data and code prefetching. Continuously-gathered profile information might enable prefetch instructions to pull data from main memory into caches in a timely fashion
<li>Instruction scheduling. If execution unit utilisation is low for a piece of code, instruction scheduling might be re-run for that code.
<li>Loop transformations. If nested loops turn out to cause poor cache utilisation, they can be transformed into loops with better characteristics.
</ul>

<p>Many of these optimisations are difficult or impossible to perform in an emulation environment for a normal processor, since there is too little information available in most instruction-set architectures (for the purposes of argument, I include Java bytecode amongst those).

<p>The goal which all these optimisations together aim to achieve can be explained like this. Consider a computer programming language. Assume that this language can be executed in two ways, either by interpreting its statements one by one, or by compiling the whole program into machine code and executing that. Both methods of execution should produce the same result, but the compiled program will usually get there much faster. Why? Because we're removing redundant calculations.

<p>Now, consider what a dynamically-recompiling emulator does to achieve its speed. At the basic level the hosted program is run by software emulation of the original processor. 'Hot' sections of code - actually data - are made into specialised subprograms which 'process' themselves, so that the net result is the same as processing the data in the original, slow way. Except, of course, that the desired result is obtained much faster.

<h4>Write a compiler by writing an interpreter</h4>

<p>Run-time profile gathering and optimisation will allow one to write an interpreter for a language, say - or any other type of data-processing application for any data - and produce a 'compiler' for that language or application automatically. In essence this would be a system which takes the "dynamic recompilation" approach on <b>any</b> program.

<p>Let me try to back this claim up. We make the assumption that a program which runs for a long time will process the same data, or the same "sort" of data with commonly repeating sections. 

<h3>Chameleon Architecture</h3>

<p>The Chameleon architecture is designed to enable as many of the previously-mentioned optimisations as possible. It has a new instruction set designed specifically for ease of program analysis, but with a couple of concessions for code size. The main features at present are as follows:

<ul>
<li>Variable liveness information encoded in the instruction stream.
<li>No branch instructions. Code is organised in basic blocks, which are tagged with various meta-data
<li>No addresses. Other basic blocks are referred to by references. All data is relocatable.
<li>No special condition code register(s). Like Alpha, normal registers are used to hold condition codes.
<li>32-bit registers with additional tags. Registers know if they are alive at a particular point in a program's execution. There are per-register carry and overflow flags to deal with multi-precision arithmetic. This allows one 'type' to be used for dataflow analysis throughout, hopefully.
<li>Multi-register load & store instructions. Modelled after the ARM, but only allowing one contiguous range of registers to be stored. Reduces stack frame manipulation code to one or two instructions, without loss of generality.
<li>64 registers, most of which are general-purpose.
<li>Four sets of fully-shadowing registers for cheap exception handling.
</ul>

<p>It's undecided at present how profiling hardware should behave, this is the subject of further research. It's anticipated that either special cache or an area of main memory might be set aside for gathering profile data on running code, with as little overhead as possible.

<hr>

<font size=-1>&copy; <a href="mailto:jules@dynarec.com">Julian Brown</a>, 2001</font>
